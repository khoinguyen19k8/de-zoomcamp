{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d1347b-1a4b-459e-a14d-688f121d19b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3d605ea-5807-4166-a9cd-616df9336af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    user = \"root\"\n",
    "    password = \"root\"\n",
    "    host = \"localhost\"\n",
    "    port = 5432\n",
    "    db = \"ny_taxi\"\n",
    "    table_name = \"taxi_zones\"\n",
    "    url = \"https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\"\n",
    "    \n",
    "    # the backup files are gzipped, and it's important to keep the correct extension\n",
    "    # for pandas to be able to open the file\n",
    "    if url.endswith('.csv.gz'):\n",
    "        csv_name = 'output.csv.gz'\n",
    "    else:\n",
    "        csv_name = 'output.csv'\n",
    "\n",
    "    os.system(f\"wget {url} -O {csv_name}\")\n",
    "\n",
    "    engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\n",
    "\n",
    "    df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)\n",
    "\n",
    "    df = next(df_iter)\n",
    "\n",
    "    # df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "    # df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "\n",
    "    df.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\n",
    "\n",
    "    df.to_sql(name=table_name, con=engine, if_exists='append')\n",
    "\n",
    "\n",
    "    while True: \n",
    "\n",
    "        try:\n",
    "            t_start = time()\n",
    "            \n",
    "            df = next(df_iter)\n",
    "\n",
    "            # df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "            # df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "\n",
    "            df.to_sql(name=table_name, con=engine, if_exists='append')\n",
    "\n",
    "            t_end = time()\n",
    "\n",
    "            print('inserted another chunk, took %.3f second' % (t_end - t_start))\n",
    "\n",
    "        except StopIteration:\n",
    "            print(\"Finished ingesting data into the postgres database\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad799f3c-4f09-4a14-9f46-0cd7651b9ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-01-18 20:47:44--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.169.168, 52.216.30.86, 54.231.135.120, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.169.168|:443... connected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished ingesting data into the postgres database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘output.csv’\n",
      "\n",
      "     0K .......... ..                                         100%  110M=0s\n",
      "\n",
      "2023-01-18 20:47:45 (110 MB/s) - ‘output.csv’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
