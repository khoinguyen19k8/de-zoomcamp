{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e146fd66-062f-4943-ad99-1899d0b1cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab5df33-83f4-4b5c-b6bb-d4bc2607a24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/03 14:51:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7806e465-8374-43f0-9c00-a0812935c357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808fe278-d152-464a-a912-3376b0b29268",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([ \n",
    "    types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True), \n",
    "    types.StructField('PULocationID', types.IntegerType(), True), \n",
    "    types.StructField('DOLocationID', types.IntegerType(), True), \n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe0b7c9-f55e-4f03-9200-7136200e771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"fhvhv_tripdata_2021-06.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6611c99-478b-404c-9239-67d567848b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89093e26-9d54-41fa-ba16-7ff3e2b70671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=============================>                             (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/03 14:52:13 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhvhv/2021/06/', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "802a7e7b-cf72-4651-a22f-6b3ccdf227b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----------------------------------+\n",
      "|pickup_datetime    |dropoff_datetime   |travel_time                        |\n",
      "+-------------------+-------------------+-----------------------------------+\n",
      "|2021-06-04 02:34:00|2021-06-04 02:40:36|INTERVAL '0 00:06:36' DAY TO SECOND|\n",
      "|2021-06-01 17:43:52|2021-06-01 17:51:20|INTERVAL '0 00:07:28' DAY TO SECOND|\n",
      "|2021-06-04 15:55:56|2021-06-04 16:18:28|INTERVAL '0 00:22:32' DAY TO SECOND|\n",
      "|2021-06-03 22:54:55|2021-06-03 23:02:51|INTERVAL '0 00:07:56' DAY TO SECOND|\n",
      "|2021-06-04 16:28:35|2021-06-04 17:00:47|INTERVAL '0 00:32:12' DAY TO SECOND|\n",
      "|2021-06-01 18:25:31|2021-06-01 18:45:22|INTERVAL '0 00:19:51' DAY TO SECOND|\n",
      "|2021-06-03 16:51:43|2021-06-03 16:58:59|INTERVAL '0 00:07:16' DAY TO SECOND|\n",
      "|2021-06-02 15:52:18|2021-06-02 16:59:36|INTERVAL '0 01:07:18' DAY TO SECOND|\n",
      "|2021-06-04 07:39:11|2021-06-04 07:50:57|INTERVAL '0 00:11:46' DAY TO SECOND|\n",
      "|2021-06-04 18:58:42|2021-06-04 19:05:48|INTERVAL '0 00:07:06' DAY TO SECOND|\n",
      "|2021-06-03 22:35:33|2021-06-03 22:47:34|INTERVAL '0 00:12:01' DAY TO SECOND|\n",
      "|2021-06-01 20:29:26|2021-06-01 20:38:37|INTERVAL '0 00:09:11' DAY TO SECOND|\n",
      "|2021-06-02 06:04:46|2021-06-02 06:37:05|INTERVAL '0 00:32:19' DAY TO SECOND|\n",
      "|2021-06-04 19:21:03|2021-06-04 20:10:58|INTERVAL '0 00:49:55' DAY TO SECOND|\n",
      "|2021-06-01 12:29:52|2021-06-01 12:38:26|INTERVAL '0 00:08:34' DAY TO SECOND|\n",
      "|2021-06-04 22:59:04|2021-06-04 23:05:45|INTERVAL '0 00:06:41' DAY TO SECOND|\n",
      "|2021-06-02 22:36:12|2021-06-02 22:46:33|INTERVAL '0 00:10:21' DAY TO SECOND|\n",
      "|2021-06-02 10:09:16|2021-06-02 10:30:29|INTERVAL '0 00:21:13' DAY TO SECOND|\n",
      "|2021-06-04 13:47:06|2021-06-04 14:16:20|INTERVAL '0 00:29:14' DAY TO SECOND|\n",
      "|2021-06-04 21:57:18|2021-06-04 22:25:44|INTERVAL '0 00:28:26' DAY TO SECOND|\n",
      "+-------------------+-------------------+-----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn(\"travel_time\", df[\"dropoff_datetime\"] - df[\"pickup_datetime\"]) \\\n",
    "    .select([\"pickup_datetime\", \"dropoff_datetime\", \"travel_time\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f01d089-fbbb-40ce-bd5c-7af99f8ae4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df \\\n",
    "    .withColumn(\"travel_time\", df[\"dropoff_datetime\"] - df[\"pickup_datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32148303-cdc4-4192-b05c-51dfc493597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.parquet('zones/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b7aae-dbe9-4c02-b827-b777528dad7a",
   "metadata": {},
   "source": [
    "# SQL editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba0dd4e4-ca9e-4911-8015-290c812eb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('fhvhv_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fdad2070-1e5a-4e60-9e20-6e457a968c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.createOrReplaceTempView('zone_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57c9f1f4-a121-4edd-b7a2-c94e0953fc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  452470|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(*)\n",
    "FROM \n",
    "    fhvhv_data\n",
    "WHERE\n",
    "    DATE_TRUNC('day', pickup_datetime) = '2021-06-15'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9a0b058-4de6-43ec-ad3c-bd238e341b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 77:======================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|max(travel_time_hours)|\n",
      "+----------------------+\n",
      "|             66.878889|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "WITH temp_table AS\n",
    "(\n",
    "    SELECT \n",
    "        pickup_datetime, dropoff_datetime, DATEDIFF(second, pickup_datetime, dropoff_datetime) / 3600.0 AS travel_time_hours\n",
    "    FROM\n",
    "        fhvhv_data\n",
    ")\n",
    "SELECT\n",
    "    MAX(travel_time_hours)\n",
    "FROM\n",
    "    temp_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "042e2ee7-d9ae-49ad-85b2-bf3b1bc4362b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 83:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|max(travel_time_hours)|\n",
      "+----------------------+\n",
      "|    19.980833333336665|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "WITH temp_table AS\n",
    "(\n",
    "    SELECT \n",
    "        pickup_datetime, \n",
    "        dropoff_datetime, \n",
    "        (DATE_PART('HOUR', travel_time) + (DATE_PART('MINUTE', travel_time) / 60) + (DATE_PART('SECOND', travel_time) / 3600)) AS travel_time_hours\n",
    "    FROM\n",
    "        fhvhv_data\n",
    ")\n",
    "SELECT\n",
    "    MAX(travel_time_hours)\n",
    "FROM\n",
    "    temp_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f65e21f3-79bd-4e1f-9d84-ccf60fb79790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['locationid', 'borough', 'zone', 'service_zone']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "49293b1e-f4d9-4047-bded-e392d54012c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag',\n",
       " 'Affiliated_base_number',\n",
       " 'travel_time']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d82f17a-1efc-458f-b6c4-b55265b3a09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 126:================================>                       (7 + 5) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------------+------------+\n",
      "|locationid| borough|               zone|service_zone|\n",
      "+----------+--------+-------------------+------------+\n",
      "|        61|Brooklyn|Crown Heights North|   Boro Zone|\n",
      "+----------+--------+-------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    zone_data.*\n",
    "FROM\n",
    "    zone_data\n",
    "WHERE \n",
    "    zone_data.locationid = \n",
    "        (\n",
    "        SELECT\n",
    "            locationid\n",
    "        FROM\n",
    "            fhvhv_data\n",
    "        JOIN\n",
    "            zone_data\n",
    "        ON \n",
    "            fhvhv_data.PULocationID = zone_data.locationid\n",
    "        GROUP BY\n",
    "            locationid\n",
    "        ORDER BY \n",
    "            COUNT(locationid) DESC\n",
    "        LIMIT 1\n",
    "        )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a96ad416-d5fc-4b79-a379-41407250f6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|locationid|count(locationid)|\n",
      "+----------+-----------------+\n",
      "|        61|           231279|\n",
      "|        79|           221244|\n",
      "|       132|           188867|\n",
      "|        37|           187929|\n",
      "|        76|           186780|\n",
      "|       231|           164344|\n",
      "|       138|           161596|\n",
      "|       234|           158937|\n",
      "|       249|           154698|\n",
      "|         7|           152493|\n",
      "|       148|           151020|\n",
      "|        68|           147673|\n",
      "|        42|           146402|\n",
      "|       255|           143683|\n",
      "|       181|           143594|\n",
      "|       225|           141427|\n",
      "|        48|           139611|\n",
      "|       246|           139431|\n",
      "|        17|           138428|\n",
      "|       170|           137879|\n",
      "+----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    locationid, COUNT(locationid)\n",
    "FROM\n",
    "    fhvhv_data\n",
    "JOIN\n",
    "    zone_data\n",
    "ON \n",
    "    fhvhv_data.PULocationID = zone_data.locationid\n",
    "GROUP BY\n",
    "    locationid\n",
    "ORDER BY \n",
    "    COUNT(locationid) DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2385d1af-4435-4aab-a6a3-668671972e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|PULocationID|count(PULocationID)|\n",
      "+------------+-------------------+\n",
      "|          61|             231279|\n",
      "|          79|             221244|\n",
      "|         132|             188867|\n",
      "|          37|             187929|\n",
      "|          76|             186780|\n",
      "|         231|             164344|\n",
      "|         138|             161596|\n",
      "|         234|             158937|\n",
      "|         249|             154698|\n",
      "|           7|             152493|\n",
      "|         148|             151020|\n",
      "|          68|             147673|\n",
      "|          42|             146402|\n",
      "|         255|             143683|\n",
      "|         181|             143594|\n",
      "|         225|             141427|\n",
      "|          48|             139611|\n",
      "|         246|             139431|\n",
      "|          17|             138428|\n",
      "|         170|             137879|\n",
      "+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    PULocationID, COUNT(PULocationID)\n",
    "FROM\n",
    "    fhvhv_data\n",
    "JOIN\n",
    "    zone_data\n",
    "ON \n",
    "    fhvhv_data.PULocationID = zone_data.locationid\n",
    "GROUP BY\n",
    "    PULocationID\n",
    "ORDER BY \n",
    "    COUNT(PULocationID) DESC\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
